{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19aa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# Please note that the datasets used in this Notebook are not supplied due to privacy.\n",
    "# The cleaning pipeline and dataset observation logs, however, are available. \n",
    "# The code can be repurposed with your own dataset considering its specifics.\n",
    "# ====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395702f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# --- Innitiation ---\n",
    "# =====================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "# --- Global Configuration ---\n",
    "# Suppress simple warnings for a cleaner output.\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# --- Constants ---\n",
    "# String that indicates a non-informative observation.\n",
    "OBSERVATION_STRING: str = \"Nenhuma observação fornecida\"\n",
    "\n",
    "# --- File Paths ---\n",
    "# important note: the input file used in this study has been preliminary cleaned and the purpose of this script is to prepare it for specific tasks\n",
    "INPUT_FILE: str = 'logs_precleaned.csv'\n",
    "OUTPUT_FILE: str = 'all_logs.csv'\n",
    "GLOSSARY_OUTPUT_FILE: str = 'farm_id_glossary.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aecef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# --- Helper Functions ---\n",
    "# =====================================================================================\n",
    "def preprocess_maintenance_logs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans, filters, and prepares maintenance log data for analysis.\n",
    "\n",
    "    This function performs the following sequential steps:\n",
    "    1.  Removes a predefined list of columns and any fully empty columns.\n",
    "    2.  Filters records to keep only those where 'Age at Event' is >= 1.\n",
    "    3.  Removes infrastructure logs (where 'Turbine No' is null).\n",
    "    4.  Filters out logs with missing component names.\n",
    "    5.  Renames specific rotor blade components for granular analysis.\n",
    "    6.  Cleans text fields ('Description', 'Observations') using regex.\n",
    "    7.  Removes logs deemed uninformative based on text content.\n",
    "    8.  Converts 'Event Date' to datetime objects and sorts the data.\n",
    "\n",
    "    Args:\n",
    "        df: The raw pandas DataFrame of maintenance logs.\n",
    "\n",
    "    Returns:\n",
    "        A fully preprocessed and sorted pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"--- 2. Preprocessing Data ---\")\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 1. Remove specified and fully empty columns.\n",
    "    cols_to_drop = ['Farm Number', 'Description_EN', 'Observations_EN']\n",
    "    df_copy.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "    df_copy.dropna(axis='columns', how='all', inplace=True)\n",
    "    print(\"Removed specified and fully empty columns.\")\n",
    "\n",
    "    # 2. Filter out rows with invalid 'Age at Event' (< 1).\n",
    "    initial_rows = len(df_copy)\n",
    "    # Ensure the column is numeric, converting errors to NaN, then drop NaNs.\n",
    "    df_copy['Age at Event'] = pd.to_numeric(df_copy['Age at Event'], errors='coerce')\n",
    "    df_copy.dropna(subset=['Age at Event'], inplace=True)\n",
    "    df_copy = df_copy[df_copy['Age at Event'] >= 1]\n",
    "    print(f\"Removed {initial_rows - len(df_copy)} rows with invalid 'Age at Event' (< 1).\")\n",
    "\n",
    "    # 3. Remove infrastructure logs.\n",
    "    initial_rows = len(df_copy)\n",
    "    df_copy.dropna(subset=['Turbine No'], inplace=True)\n",
    "    print(f\"Removed {initial_rows - len(df_copy)} infrastructure logs.\")\n",
    "\n",
    "    # 4. Filter out entries with no component name.\n",
    "    df_copy.dropna(subset=['Component Name'], inplace=True)\n",
    "    df_copy = df_copy[df_copy['Component Name'] != '']\n",
    "\n",
    "    # 5. Rename rotor blade components.\n",
    "    component_name_map = {\n",
    "        'Rotor Blade System 1': 'Rotor Blade No. 1',\n",
    "        'Rotor Blade System 2': 'Rotor Blade No. 2',\n",
    "        'Rotor Blade System 3': 'Rotor Blade No. 3',\n",
    "    }\n",
    "    df_copy['Component Name'] = df_copy['Component Name'].replace(component_name_map)\n",
    "    df_copy = df_copy[df_copy['Component Name'] != 'Rotor Blades Overall']\n",
    "    print(\"Standardised component names.\")\n",
    "\n",
    "    # 6. Clean text fields.\n",
    "    df_copy[['Description', 'Observations']] = df_copy[['Description', 'Observations']].fillna('')\n",
    "    wo_pattern = r'\\(WO\\s\\d+\\)'\n",
    "    log_code_pattern = r'\\b\\d{2}\\.\\d{3}\\.\\d{2}-\\d{4}\\b'\n",
    "    multiple_spaces_pattern = r'\\s+'\n",
    "    for col in ['Description', 'Observations']:\n",
    "        df_copy[col] = (df_copy[col].astype(str)\n",
    "                        .str.replace(wo_pattern, '', regex=True)\n",
    "                        .str.replace(log_code_pattern, '', regex=True)\n",
    "                        .str.replace(multiple_spaces_pattern, ' ', regex=True)\n",
    "                        .str.strip())\n",
    "    \n",
    "    # 7. Remove uninformative logs.\n",
    "    initial_rows = len(df_copy)\n",
    "    empty_both = (df_copy['Description'] == '') & (df_copy['Observations'] == '')\n",
    "    short_desc = (df_copy['Description'].str.split().str.len() <= 1)\n",
    "    uninformative_obs = df_copy['Observations'].isin(['', OBSERVATION_STRING])\n",
    "    df_copy = df_copy[~(empty_both | (short_desc & uninformative_obs))]\n",
    "    print(f\"Removed {initial_rows - len(df_copy)} uninformative logs.\")\n",
    "\n",
    "    # 8. Convert to datetime and sort.\n",
    "    df_copy['Event Date'] = pd.to_datetime(df_copy['Event Date'])\n",
    "    df_copy.sort_values(by='Event Date', inplace=True, ignore_index=True)\n",
    "    print(\"Sorted data by 'Event Date'.\")\n",
    "\n",
    "    print(\"\\nData preprocessing complete.\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def anonymise_farm_ids(\n",
    "    df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Anonymises 'Farm ID' using two-letter codes and creates a glossary.\n",
    "\n",
    "    Args:\n",
    "        df: The preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - The DataFrame with the new anonymised 'Wind Farm' column.\n",
    "        - A new DataFrame serving as a glossary to trace original IDs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 3. Anonymising Farm IDs ---\")\n",
    "    \n",
    "    # Generate two-letter codes (AA, AB, ..., ZZ).\n",
    "    letters = string.ascii_uppercase\n",
    "    two_letter_codes_iter = itertools.product(letters, repeat=2)\n",
    "    anonymised_codes = [\"\".join(code) for code in two_letter_codes_iter]\n",
    "    \n",
    "    unique_farms = df['Farm ID'].unique()\n",
    "    \n",
    "    if len(unique_farms) > len(anonymised_codes):\n",
    "        raise ValueError(\n",
    "            f\"Error: {len(unique_farms)} unique farms found, but only \"\n",
    "            f\"{len(anonymised_codes)} unique two-letter codes are available.\"\n",
    "        )\n",
    "\n",
    "    # Create the mapping from unique Farm ID to an anonymised name.\n",
    "    glossary = {\n",
    "        farm_id: f'Wind Farm {anonymised_codes[i]}'\n",
    "        for i, farm_id in enumerate(unique_farms)\n",
    "    }\n",
    "    \n",
    "    # Map the anonymised names to a new 'Wind Farm' column.\n",
    "    df['Wind Farm'] = df['Farm ID'].map(glossary)\n",
    "    \n",
    "    # Reorder columns to place 'Wind Farm' next to 'Farm ID'.\n",
    "    try:\n",
    "        farm_id_loc = df.columns.get_loc('Farm ID')\n",
    "        cols = list(df.columns)\n",
    "        cols.insert(farm_id_loc + 1, cols.pop(cols.index('Wind Farm')))\n",
    "        df = df[cols]\n",
    "    except KeyError:\n",
    "        print(\"Warning: 'Farm ID' column not found. Cannot reorder.\")\n",
    "\n",
    "    # Create the glossary DataFrame.\n",
    "    glossary_df = pd.DataFrame(\n",
    "        list(glossary.items()), columns=['Farm ID', 'Anonymised ID']\n",
    "    )\n",
    "    \n",
    "    print(\"Anonymisation complete and glossary created.\")\n",
    "    return df, glossary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2162b259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading Data ---\n",
      "Successfully loaded. Initial shape: (12152, 13)\n",
      "\n",
      "--- 2. Preprocessing Data ---\n",
      "Removed specified and fully empty columns.\n",
      "Removed 3 rows with invalid 'Age at Event' (< 1).\n",
      "Removed 473 infrastructure logs.\n",
      "Standardised component names.\n",
      "Removed 4 uninformative logs.\n",
      "Sorted data by 'Event Date'.\n",
      "\n",
      "Data preprocessing complete.\n",
      "\n",
      "--- 3. Anonymising Farm IDs ---\n",
      "Anonymisation complete and glossary created.\n",
      "\n",
      "--- 4. Saving Outputs ---\n",
      "Final dataset shape: (10926, 11)\n",
      "Saved the analysis-ready dataset to 'all_logs.csv'.\n",
      "Saved the farm ID glossary to 'farm_id_glossary.csv'.\n",
      "\n",
      "Process completed successfully. ✅\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# --- Main Execution Pipeline ---\n",
    "# =====================================================================================\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"--- 1. Loading Data ---\")\n",
    "try:\n",
    "    df_raw = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Successfully loaded. Initial shape: {df_raw.shape}\\n\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{INPUT_FILE}'.\")\n",
    "    df_raw = None\n",
    "\n",
    "if df_raw is not None:\n",
    "    # 2. Run the full preprocessing pipeline\n",
    "    df_processed = preprocess_maintenance_logs(df_raw)\n",
    "\n",
    "    # 3. Anonymise farm IDs and create the glossary\n",
    "    df_final, df_glossary = anonymise_farm_ids(df_processed)\n",
    "\n",
    "    # 4. Save the final outputs\n",
    "    print(\"\\n--- 4. Saving Outputs ---\")\n",
    "    df_final.to_csv(OUTPUT_FILE, index=False)\n",
    "    df_glossary.to_csv(GLOSSARY_OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(f\"Final dataset shape: {df_final.shape}\")\n",
    "    print(f\"Saved the analysis-ready dataset to '{OUTPUT_FILE}'.\")\n",
    "    print(f\"Saved the farm ID glossary to '{GLOSSARY_OUTPUT_FILE}'.\")\n",
    "    \n",
    "    print(\"\\nProcess completed successfully. ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All necessary data files loaded successfully.\n",
      "\n",
      "## Top 10 Most Frequently Occurring Component Names\n",
      "Component Name\n",
      "Rotor Blades                     1817\n",
      "Converter System Generator 1     1065\n",
      "Tower System Bottom Section       800\n",
      "Central Hydraulic System          693\n",
      "Turbine Control System            681\n",
      "Generator 1                       627\n",
      "Rotor Hub Unit                    565\n",
      "Tower System Overall              552\n",
      "MV-Transformation Generator 1     445\n",
      "Gearbox                           412\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "## Top 10 Most Frequent Wind Farms (Normalised)\n",
      "Monitoring Period: 18.57 years (from 2006-01-01 to 2024-07-28)\n",
      "   Anonymised ID  Normalised Frequency (Events/Turbine/Year)\n",
      "14  Wind Farm AP                                    4.325778\n",
      "6   Wind Farm BB                                    3.607806\n",
      "0   Wind Farm AN                                    2.977641\n",
      "21  Wind Farm AW                                    2.943683\n",
      "10  Wind Farm BG                                    2.940093\n",
      "12  Wind Farm AC                                    2.789319\n",
      "13  Wind Farm AR                                    2.735471\n",
      "4   Wind Farm AO                                    2.680854\n",
      "8   Wind Farm AY                                    2.500079\n",
      "22  Wind Farm AE                                    2.494951\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "## Top 5 Most Frequently Failing Turbines (Normalised by Monitoring Period)\n",
      "  Unique Turbine ID  Normalised Frequency (Events/Year)\n",
      "0         AP-WT-003                            4.954003\n",
      "1         AP-WT-002                            4.307828\n",
      "2         AE-WT-001                            3.984741\n",
      "3         AO-WT-002                            3.877046\n",
      "4         AN-WT-015                            3.823198\n",
      "## Wind Farms with the Highest Number of Maintenance Logs (Raw Count)\n",
      "   Anonymised ID  Event Count  Num Turbines\n",
      "0   Wind Farm AN         2046            37\n",
      "1   Wind Farm AH         1513            38\n",
      "2   Wind Farm AT          769            20\n",
      "3   Wind Farm AM          745            24\n",
      "4   Wind Farm AO          697            14\n",
      "5   Wind Farm AX          385            33\n",
      "6   Wind Farm BB          335             5\n",
      "7   Wind Farm AK          328            10\n",
      "8   Wind Farm AY          325             7\n",
      "9   Wind Farm AL          275             8\n",
      "10  Wind Farm BG          273             5\n",
      "11  Wind Farm AD          272             8\n",
      "12  Wind Farm AC          259             5\n",
      "13  Wind Farm AR          254             5\n",
      "14  Wind Farm AP          241             3\n",
      "15  Wind Farm BD          218            15\n",
      "16  Wind Farm AI          213             8\n",
      "17  Wind Farm AA          201             9\n",
      "18  Wind Farm AB          186            10\n",
      "19  Wind Farm AG          183             4\n",
      "20  Wind Farm AQ          178             4\n",
      "21  Wind Farm AW          164             3\n",
      "22  Wind Farm AE          139             3\n",
      "23  Wind Farm AS          118             5\n",
      "24  Wind Farm AZ          116             6\n",
      "25  Wind Farm BA          105             5\n",
      "26  Wind Farm BI           78             6\n",
      "27  Wind Farm AV           66             3\n",
      "28  Wind Farm BH           53             5\n",
      "29  Wind Farm BE           39             2\n",
      "30  Wind Farm BC           36             1\n",
      "31  Wind Farm AJ           36             1\n",
      "32  Wind Farm AF           30             1\n",
      "33  Wind Farm BF           27             5\n",
      "34  Wind Farm AU           22             1\n",
      "35  Wind Farm BJ            1             2\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# --- Overview of the Data ---\n",
    "# =====================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Required Data ---\n",
    "try:\n",
    "    df_final = pd.read_csv('all_logs.csv')\n",
    "    df_glossary = pd.read_csv('farm_id_glossary.csv')\n",
    "    df_portfolio = pd.read_csv('portfolio.csv')\n",
    "    print(\"✅ All necessary data files loaded successfully.\\n\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: Could not find a required file. Please check file paths.\")\n",
    "    print(f\"Details: {e}\")\n",
    "    df_final = None\n",
    "\n",
    "if df_final is not None:\n",
    "    # Ensure 'Event Date' is in datetime format for calculations.\n",
    "    df_final['Event Date'] = pd.to_datetime(df_final['Event Date'])\n",
    "\n",
    "    # --- 2. Top 10 Most Frequent Component Names (with Rotor Blades aggregated) ---\n",
    "    print(\"## Top 10 Most Frequently Occurring Component Names\")\n",
    "    \n",
    "    # Temporarily group all 'Rotor Blade No. X' into a single 'Rotor Blades' category for this analysis.\n",
    "    component_names_grouped = df_final['Component Name'].str.replace(r'Rotor Blade No. \\d', 'Rotor Blades', regex=True)\n",
    "    top_10_components = component_names_grouped.value_counts().nlargest(10)\n",
    "    \n",
    "    print(top_10_components)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "    # --- 3. Top 10 Most Frequent Wind Farms (Normalised) ---\n",
    "    print(\"## Top 10 Most Frequent Wind Farms (Normalised)\")\n",
    "    \n",
    "    # Calculate the monitoring period in years based on the log data.\n",
    "    min_date = df_final['Event Date'].min()\n",
    "    max_date = df_final['Event Date'].max()\n",
    "    monitoring_period_years = (max_date - min_date).days / 365.25\n",
    "    print(f\"Monitoring Period: {monitoring_period_years:.2f} years (from {min_date.date()} to {max_date.date()})\")\n",
    "\n",
    "    # Prepare portfolio data using the 'TN' column.\n",
    "    df_portfolio['Num Turbines'] = df_portfolio['TN'].astype(int)\n",
    "    df_portfolio.rename(columns={'Code': 'Farm ID'}, inplace=True)\n",
    "    \n",
    "    # Merge portfolio and glossary to link anonymised names with turbine counts.\n",
    "    farm_info = pd.merge(df_portfolio[['Farm ID', 'Num Turbines']], df_glossary, on='Farm ID')\n",
    "    \n",
    "    # Count maintenance events for each anonymised farm name.\n",
    "    farm_event_counts = df_final['Wind Farm'].value_counts().reset_index()\n",
    "    farm_event_counts.columns = ['Anonymised ID', 'Event Count']\n",
    "    \n",
    "    # Merge event counts with the farm information.\n",
    "    farm_stats = pd.merge(farm_event_counts, farm_info, on='Anonymised ID')\n",
    "    \n",
    "    # Calculate the normalised failure frequency.\n",
    "    farm_stats['Normalised Frequency (Events/Turbine/Year)'] = \\\n",
    "        (farm_stats['Event Count'] / farm_stats['Num Turbines']) / monitoring_period_years\n",
    "        \n",
    "    # Display the top 10.\n",
    "    top_2_farms = farm_stats.sort_values(\n",
    "        by='Normalised Frequency (Events/Turbine/Year)', ascending=False\n",
    "    ).head(10)\n",
    "    print(top_2_farms[['Anonymised ID', 'Normalised Frequency (Events/Turbine/Year)']])\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "\n",
    "    # --- 4. Top 5 Most Frequent Turbines (with Unique IDs, Normalised) ---\n",
    "    print(\"## Top 5 Most Frequently Failing Turbines (Normalised by Monitoring Period)\")\n",
    "    \n",
    "    # Create a unique ID for each turbine by combining the farm's two-letter code and the turbine number.\n",
    "    farm_code = df_final['Wind Farm'].str.split().str[-1]\n",
    "    df_final['Unique Turbine ID'] = farm_code + '-' + df_final['Turbine No']\n",
    "    \n",
    "    # Count events per unique turbine ID.\n",
    "    turbine_event_counts = df_final['Unique Turbine ID'].value_counts()\n",
    "    \n",
    "    # Normalise by the monitoring period.\n",
    "    normalised_turbine_freq = turbine_event_counts / monitoring_period_years\n",
    "    \n",
    "    # Display the top 5.\n",
    "    top_5_turbines = normalised_turbine_freq.nlargest(5).reset_index()\n",
    "    top_5_turbines.columns = ['Unique Turbine ID', 'Normalised Frequency (Events/Year)']\n",
    "    print(top_5_turbines)\n",
    "\n",
    "    # --- 5. Wind Farms with the Highest Number of Logs (Raw Count) ---\n",
    "    print(\"## Wind Farms with the Highest Number of Maintenance Logs (Raw Count)\")\n",
    "\n",
    "    # Check if the farm_stats DataFrame exists from the previous cell.\n",
    "    if 'farm_stats' in locals() and isinstance(farm_stats, pd.DataFrame):\n",
    "        \n",
    "        # Sort the DataFrame by the 'Event Count' column in descending order.\n",
    "        top_farms_by_raw_count = farm_stats.sort_values(\n",
    "            by='Event Count', ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        # Select and display the relevant columns.\n",
    "        print(top_farms_by_raw_count[['Anonymised ID', 'Event Count', 'Num Turbines']])\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Error: The 'farm_stats' DataFrame was not found. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f14c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created subset for component: 'Converter System Generator 1'\n",
      "Shape of the new subset: (1065, 5)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# --- Creating Subsets for Specific Tasks ---\n",
    "# =====================================================================================\n",
    "\n",
    "# --- Subset by Component Name ---\n",
    "\n",
    "target_component = 'Converter System Generator 1'\n",
    "\n",
    "# Define the columns to keep for this subset.\n",
    "columns_to_keep = [\n",
    "    'Wind Farm', \n",
    "    'Unique Turbine ID', \n",
    "    'Description', \n",
    "    'Observations', \n",
    "    'Age at Event'\n",
    "]\n",
    "\n",
    "# Create the subset\n",
    "# Note: This assumes 'Unique Turbine ID' was created in the previous step.\n",
    "# If not, we create it here to be safe.\n",
    "if 'Unique Turbine ID' not in df_final.columns:\n",
    "    farm_code = df_final['Wind Farm'].str.split().str[-1]\n",
    "    df_final['Unique Turbine ID'] = farm_code + '-' + df_final['Turbine No']\n",
    "    \n",
    "df_component_subset = df_final[df_final['Component Name'] == target_component][columns_to_keep]\n",
    "\n",
    "print(f\"✅ Created subset for component: '{target_component}'\")\n",
    "print(f\"Shape of the new subset: {df_component_subset.shape}\")\n",
    "df_component_subset.to_csv('converter_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created subset for farm 'Wind Farm AO' (697 rows)\n",
      "\n",
      "✅ Created subset for farm 'Wind Farm AM' (745 rows)\n"
     ]
    }
   ],
   "source": [
    "# --- Subsets for Two Specified Wind Farms ---\n",
    "\n",
    "# The names should be in the 'Wind Farm XX' format.\n",
    "target_farm_1 = 'Wind Farm AO'\n",
    "target_farm_2 = 'Wind Farm AM'\n",
    "\n",
    "# Define the columns to keep for these subsets.\n",
    "columns_to_keep = [\n",
    "    'Component Name', \n",
    "    'Description', \n",
    "    'Observations'\n",
    "]\n",
    "\n",
    "# Create the first subset.\n",
    "df_farm_1_subset = df_final[df_final['Wind Farm'] == target_farm_1][columns_to_keep]\n",
    "\n",
    "# Create the second subset.\n",
    "df_farm_2_subset = df_final[df_final['Wind Farm'] == target_farm_2][columns_to_keep]\n",
    "\n",
    "\n",
    "print(f\"✅ Created subset for farm '{target_farm_1}' ({df_farm_1_subset.shape[0]} rows)\")\n",
    "df_farm_1_subset.to_csv('farm_AO_subset.csv', index=False)\n",
    "\n",
    "print(f\"\\n✅ Created subset for farm '{target_farm_2}' ({df_farm_2_subset.shape[0]} rows)\")\n",
    "df_farm_2_subset.to_csv('farm_AM_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created subset for turbine: 'AP-WT-003'\n",
      "Shape of the new subset: (92, 6)\n"
     ]
    }
   ],
   "source": [
    "# --- Subset by Unique Turbine ID ---\n",
    "\n",
    "# The ID must be in the 'XX-WT-NNN' format.\n",
    "target_turbine_id = 'AP-WT-003'\n",
    "\n",
    "# Define the columns to keep for this subset.\n",
    "columns_to_keep = [\n",
    "    'WONUM',\n",
    "    'Component Name',\n",
    "    'Description',\n",
    "    'Observations',\n",
    "    'Age at Event',\n",
    "    'Event Date'\n",
    "]\n",
    "\n",
    "# Create the subset\n",
    "# Note: This assumes 'Unique Turbine ID' was created in a previous step.\n",
    "if 'Unique Turbine ID' not in df_final.columns:\n",
    "    farm_code = df_final['Wind Farm'].str.split().str[-1]\n",
    "    df_final['Unique Turbine ID'] = farm_code + '-' + df_final['Turbine No']\n",
    "    \n",
    "df_turbine_subset = df_final[df_final['Unique Turbine ID'] == target_turbine_id][columns_to_keep]\n",
    "\n",
    "print(f\"✅ Created subset for turbine: '{target_turbine_id}'\")\n",
    "print(f\"Shape of the new subset: {df_turbine_subset.shape}\")\n",
    "df_turbine_subset.to_csv('turbine_AP-WT-003_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98c9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created a reproducible random subset with 20.0% of the data.\n",
      "Original dataset shape: (10926, 12)\n",
      "New random subset shape: (2185, 12)\n"
     ]
    }
   ],
   "source": [
    "# --- Create a Reproducible Random 25% Subset ---\n",
    "\n",
    "# The `random_state` ensures that the random sample is always the same.\n",
    "subset_fraction = 0.20\n",
    "reproducibility_seed = 42\n",
    "\n",
    "# Create the random subset using the .sample() method.\n",
    "df_random_subset = df_final.sample(\n",
    "    frac=subset_fraction, \n",
    "    random_state=reproducibility_seed\n",
    ")\n",
    "\n",
    "print(f\"✅ Created a reproducible random subset with {subset_fraction * 100}% of the data.\")\n",
    "print(f\"Original dataset shape: {df_final.shape}\")\n",
    "print(f\"New random subset shape: {df_random_subset.shape}\")\n",
    "df_random_subset.to_csv('logs_subset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
